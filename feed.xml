<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="http://kajuna0amendez.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://kajuna0amendez.github.io/" rel="alternate" type="text/html" /><updated>2020-05-31T07:50:30-05:00</updated><id>http://kajuna0amendez.github.io/feed.xml</id><title type="html">Andres Mendez-Vazquez, A Personal Exploration about Intelligent Systems</title><subtitle>Exploring subjects on Machine Learning, Deep Learning and Computational Mathematics</subtitle><entry><title type="html">About slides on ML, DL, AI, Optimization</title><link href="http://kajuna0amendez.github.io/2020/05/31/onclasse.html" rel="alternate" type="text/html" title="About slides on ML, DL, AI, Optimization" /><published>2020-05-31T00:00:00-05:00</published><updated>2020-05-31T00:00:00-05:00</updated><id>http://kajuna0amendez.github.io/2020/05/31/onclasse</id><content type="html" xml:base="http://kajuna0amendez.github.io/2020/05/31/onclasse.html">&lt;p&gt;I have been adding the slides on the subjects of 
Machine Learning, Deep Learning, Linear Algebra, Probability.
However, I need to complete some subjects in Deep Learning:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Autoencoders&lt;/li&gt;
  &lt;li&gt;Reinforcement Learning&lt;/li&gt;
  &lt;li&gt;Generative Deep Networks&lt;/li&gt;
  &lt;li&gt;Second Order Methods&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However there is a subject on their applications that I consider
way more  interesting because there we have more interesting
architectures applied to the subjects as&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Attention for Natural Language Processing,&lt;/li&gt;
  &lt;li&gt;Customer Churning,&lt;/li&gt;
  &lt;li&gt;3D reconstruction,&lt;/li&gt;
  &lt;li&gt;Metric Learning,&lt;/li&gt;
  &lt;li&gt;etc.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Also I am planning to add my slides on AI. However, I need to select the 
ones that are relevant for the times. Many things on the “old” AI are not
anymore interesting because they have been surpassed by other solutions.
However, the subject of&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bayesian Networks,&lt;/li&gt;
  &lt;li&gt;Reinforcment Learning&lt;/li&gt;
  &lt;li&gt;Approximated Dynamic Programming&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That are still valid on these times.&lt;/p&gt;

&lt;p&gt;Finally, but not least important, and something I been trying to generate for a long
tiem, a series of slides in optimization. I have several ideas, based on certain works
by Nocedall and Jordan, on how to select the subjects.&lt;/p&gt;</content><author><name></name></author><summary type="html">I have been adding the slides on the subjects of Machine Learning, Deep Learning, Linear Algebra, Probability. However, I need to complete some subjects in Deep Learning:</summary></entry><entry><title type="html">About Lea Vega graduation</title><link href="http://kajuna0amendez.github.io/2020/05/30/leavega.html" rel="alternate" type="text/html" title="About Lea Vega graduation" /><published>2020-05-30T00:00:00-05:00</published><updated>2020-05-30T00:00:00-05:00</updated><id>http://kajuna0amendez.github.io/2020/05/30/leavega</id><content type="html" xml:base="http://kajuna0amendez.github.io/2020/05/30/leavega.html">&lt;p&gt;Five years ago, I was surprised that Lea Vega went to my office to ask me to be her advisor. After all
I am not exactly a nice person… actually I am always asking for more to my students. But Lea did not 
need any “encouragment” (Do you see my terrible smile?). For example, once Dr. Paul Gader came to visit 
the center and was quite nicely impressed on her work. Now, after all this time and work by her using 
collapsed Gibbs samplers, neo4j, and a lot of data collection, she was able to finish and got good results as:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/lea_files/Performance.png&quot; alt=&quot;Performance&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Actually, it was nice to see that a deep analysis of the problem can outperform the simple application of
massive data.&lt;/p&gt;

&lt;p&gt;So, yesterday, she was finally able to present, online the first one at Cinvestav, her graduation exam, making
her a new minted PhD. She already has a nice work waiting for her, and that is good given the situation. I am 
quite happy, and now new arise from this. Two good news are&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We are planning to post her code, so anybody can repeat her experiments.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In a previous post, thoughts about sufficiency principle gave me an idea about creating something called sufficiency on-line&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;How &lt;strong&gt;regret&lt;/strong&gt; can be used in your estimator as you collect data by extending the idea from optimization to sufficiency&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">Five years ago, I was surprised that Lea Vega went to my office to ask me to be her advisor. After all I am not exactly a nice person… actually I am always asking for more to my students. But Lea did not need any “encouragment” (Do you see my terrible smile?). For example, once Dr. Paul Gader came to visit the center and was quite nicely impressed on her work. Now, after all this time and work by her using collapsed Gibbs samplers, neo4j, and a lot of data collection, she was able to finish and got good results as:</summary></entry><entry><title type="html">On the Likelihood Principle</title><link href="http://kajuna0amendez.github.io/2020/05/28/probability.html" rel="alternate" type="text/html" title="On the Likelihood Principle" /><published>2020-05-28T00:00:00-05:00</published><updated>2020-05-28T00:00:00-05:00</updated><id>http://kajuna0amendez.github.io/2020/05/28/probability</id><content type="html" xml:base="http://kajuna0amendez.github.io/2020/05/28/probability.html">&lt;p&gt;The likelihood principle formalized by Birnbaum in his famous theorem&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Theorem
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The formal likelihood principle follows from the weak 
conditional probability and sufficiency principle 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There, Birnbaum, defines a symbol called \(Ev \left( E,x \right)\) that 
tries to give a more formal view of the evidential meaning of a specified 
sample \(\left( E, x \right)\). Basically, he was able to use this symbol 
to define the essential properties of evidence defined by the evidence
of \(x\) of an experiment \(E\).&lt;/p&gt;

&lt;p&gt;Then, he went to define an equivalence between different samples on different
experiments by \(Ev\left(E, x\right) = Ev\left(E', y\right)\). This allowed to 
define a principle called of sufficiency.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The Principle of Sufficiency&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Given a experiment with outcomes \(x\) and a sufficient statistic \(t = t\left( x \right)\) 
  summarizing the basic properties of the samples \(x\) and if you have \(E'\) is
  the experiment, derived from \(E\), which uses \(t\) to represent any outcome \(x \in E\), then 
  \(Ev \left( E,x \right) Ev \left( E,t \right)\) for all \(x\).&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An interesting part of the principle of sufficiency is the part of &lt;strong&gt;derivation&lt;/strong&gt; 
because such word can signify many possibles way to derive the experiment. Thus, 
a sufficient statistics can be seen as a connection between any experiments that 
could be generated. If we decide to go Bayesian, and we have a distribution based on 
\(E'\) called \(f_{E'}\), we could say&lt;/p&gt;

\[p_{E'}\left(x\right \vert \theta') = p_{E'}\left(t \vert \theta' \right) = p_{E'}\left(t \right)\]

&lt;p&gt;Thus, if this \(t\) is found at the experiment \(E\), Is there an interest to do an extension? After all,
once we know the \(t\) we have a good summary of the data. Making any interest on making an extension of \(E\) 
to diminish greatly given the cost of such experiments. For example, you could be a company storing data
about errors in manufacturing, and you could decide to build an estimator about such errors. It seems to be that
you could be quite happy given that you could believe that such estimator is sufficient. However, somebody, as
always, comments that given that the the production lines do not repeat models, How such sufficient statistic can exist?
Yes, it exist for the past and present data samples, but not for the future data samples.  Such situation is 
quite problematic, given that the &lt;strong&gt;Holly Grail&lt;/strong&gt; of estimation, from the Gaussian point of view, 
is to be able to summarize the data.&lt;/p&gt;

&lt;p&gt;Actually, Pitman–Koopman–Darmois theorem points to such problem. This theorem restricts sufficiency to the 
exponential family. But as anybody can testify, chaotic events, tend to point to a non-Gaussian Universe i.e the variance
simply is not bounded. A natural question arises, under such situations, How well the Likelihood principle can work? 
Is there a way to measure &lt;strong&gt;well&lt;/strong&gt;? A fascinating problem to say the least.&lt;/p&gt;</content><author><name></name></author><summary type="html">The likelihood principle formalized by Birnbaum in his famous theorem</summary></entry><entry><title type="html">The Beginning</title><link href="http://kajuna0amendez.github.io/2020/05/26/Machine-Learning.html" rel="alternate" type="text/html" title="The Beginning" /><published>2020-05-26T00:00:00-05:00</published><updated>2020-05-26T00:00:00-05:00</updated><id>http://kajuna0amendez.github.io/2020/05/26/Machine-Learning</id><content type="html" xml:base="http://kajuna0amendez.github.io/2020/05/26/Machine-Learning.html">&lt;h1 id=&quot;long-ago-evolution-gave-us-the-enchanted-loom-now-we-are-looking-for-a-companion-beyond-de-lonely-universe&quot;&gt;“Long ago evolution gave us the enchanted loom, now we are looking for a companion beyond de lonely universe”&lt;/h1&gt;

&lt;p&gt;My intention with this page is to post my thoughts, classes, notes and projects about these ideas:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- Machine Learning
- Deep Learning
- Artificial Intelligence
- Computational Mathematics 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Because at the end the search for Artificial Intelligence is to use applied mathematics to understand the universe. Therefore, I am plannign to 
add thoughts and mathematical equations about these issues.&lt;/p&gt;</content><author><name></name></author><summary type="html">“Long ago evolution gave us the enchanted loom, now we are looking for a companion beyond de lonely universe”</summary></entry></feed>