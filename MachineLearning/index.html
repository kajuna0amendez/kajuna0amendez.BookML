<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Introduction to Machine Learning | Andres Mendez-Vazquez, A Personal Exploration about Intelligent Systems</title>
<meta name="generator" content="Jekyll v4.0.1" />
<meta property="og:title" content="Introduction to Machine Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Exploring subjects on Machine Learning, Deep Learning and Computational Mathematics" />
<meta property="og:description" content="Exploring subjects on Machine Learning, Deep Learning and Computational Mathematics" />
<link rel="canonical" href="http://localhost:4000/MachineLearning/" />
<meta property="og:url" content="http://localhost:4000/MachineLearning/" />
<meta property="og:site_name" content="Andres Mendez-Vazquez, A Personal Exploration about Intelligent Systems" />
<script type="application/ld+json">
{"@type":"WebPage","url":"http://localhost:4000/MachineLearning/","headline":"Introduction to Machine Learning","description":"Exploring subjects on Machine Learning, Deep Learning and Computational Mathematics","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Andres Mendez-Vazquez, A Personal Exploration about Intelligent Systems" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Andres Mendez-Vazquez, A Personal Exploration about Intelligent Systems</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/MachineLearning/">Introduction to Machine Learning</a><a class="page-link" href="/AdvancedMachineLearning/">Advanced Machine Learning</a><a class="page-link" href="/MLBusiness/">Machine Learning and Business</a><a class="page-link" href="/DeepLearning/">Deep Learning</a><a class="page-link" href="/Manifoldlearning/">Manifold Learning and Information Geometry</a><a class="page-link" href="/ArtificialIntelligence/">Introduction to Artificial Intelligence</a><a class="page-link" href="/AnalysisAlgorithms/">Analysis of Algorithms</a><a class="page-link" href="/LinearAlgebra/">Linear Algebra for AI</a><a class="page-link" href="/Probability/">Probability for AI</a><a class="page-link" href="/Optimization/">Optimization for AI</a><a class="page-link" href="/Cython/">Cython and Numpy programming</a><a class="page-link" href="/Projects/">Personal Projects</a><a class="page-link" href="/TechnicalPapers/">Technical Papers</a><a class="page-link" href="/Articles/">Publications</a><a class="page-link" href="/Students/">Past Students</a><a class="page-link" href="/Bio/">About Us</a></div>
      </nav></div>
</header>

<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Introduction to Machine Learning</h1>
  </header>

  <div class="post-content">
    <p>These are my attempts to write a series of slides on the many topic of ML.</p>

<ol>
  <li><strong>Introduction</strong> <a href="/assets/ml_files/01_Introduction.pdf">slides</a>
    <ul>
      <li>Why Learning?</li>
    </ul>

\[\]
  </li>
  <li><strong>The Basic Ideas of Learning</strong> <a href="/assets/ml_files/02_A_Basic_Introduction_to_Learning.pdf">slides</a>
    <ul>
      <li>Some of the basic ideas on Learning</li>
    </ul>

\[\min_{\widehat{f}}R\left(\widehat{f}\right)=\min_{\widehat{f}}E_{\mathcal{X},\mathcal{Y}}\left[\left(\widehat{f}\left(\boldsymbol{x}\right)-y\right)^{2}\vert\boldsymbol{x}\in\mathcal{X}\subseteq\mathbb{R}^{d},y\in\mathcal{Y}\subseteq\mathbb{R}\right]\]
  </li>
  <li><strong>Linear Models</strong> <a href="/assets/ml_files/03_Linear_Models_and_Least_Squared_Error.pdf">slides</a>
    <ul>
      <li>A basic introduction to Linear models</li>
      <li>Some Basic ideas on regularization</li>
      <li>Interludes with Linear Algebra and Calculus</li>
    </ul>

\[g\left(\boldsymbol{\boldsymbol{x}}\right)=\boldsymbol{W^{T}\boldsymbol{x}=\boldsymbol{T}^{T}\left(\boldsymbol{X}^{+}\right)^{T}\boldsymbol{x}}\]
  </li>
  <li><strong>Regularization</strong> <a href="/assets/ml_files/04_Regularization_Gradient_Descent_Fisher_Linear_Discriminant.pdf">slides</a>
    <ul>
      <li>A deeper study in the field of regularization</li>
    </ul>

\[C_{h}=\left(A^{T}A+h^{2}I\right)^{-1}A^{T}\]
  </li>
  <li><strong>Batch and Stochastic Gradient Descent</strong> <a href="/assets/ml_files/05_Stochastic_Gradient_Descent.pdf">slides</a>
    <ul>
      <li>Batch Gradient Descent</li>
      <li>Accelerating Gradient Descent</li>
      <li>Stochastic Gradient Descent</li>
      <li>Minbatch</li>
      <li>Regret in Machine Learning</li>
      <li>AdaGrad</li>
      <li>ADAM</li>
    </ul>

\[\boldsymbol{w}_{n}=\boldsymbol{w}_{n-1}+\mu_{n}\boldsymbol{x}_{n}\left(\boldsymbol{x}_{n}^{T}\boldsymbol{w}_{n-1}-y_{n}\right)\]
  </li>
  <li><strong>Logistic Regression</strong> <a href="/assets/ml_files/06_Logistic_Regression.pdf">slides</a>
    <ul>
      <li>Interlude with Generative vs Discriminative models</li>
      <li>The Logistic Regression model</li>
      <li>Accelerating the logistic regression</li>
    </ul>

\[\mathcal{L}\left(\boldsymbol{w}\right)=\sum_{i=1}^{N}\left\{ y_{i}\boldsymbol{w}^{T}\boldsymbol{x}_{i}-\log\left(1+\exp\left\{ \boldsymbol{w}^{T}\boldsymbol{x}_{i}\right\} \right)\right\}\]
  </li>
  <li><strong>Introduction to Bayes Classification</strong>  <a href="/assets/ml_files/07_Introduction_Bayes_Classification.pdf">slides</a>
    <ul>
      <li>Naive Bayes</li>
      <li>Discriminative Functions</li>
    </ul>

\[\ln L\left(\omega_{i}\right)=-\frac{n}{2}\ln\left|\Sigma_{i}\right|-\frac{1}{2}\left[\sum_{j=1}^{n}\left(\boldsymbol{x_{j}}-\boldsymbol{\mu_{i}}\right)^{T}\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}}-\boldsymbol{\mu_{i}}\right)\right]+c_{2}\]
  </li>
  <li><strong>Maximum a Posteriori Methods</strong>
    <ul>
      <li>Going beyond Maximum Likelihood</li>
      <li>The General Case</li>
      <li>How can be used in Bayesian Learning?</li>
    </ul>

\[p\left(\boldsymbol{w},\sigma^{2}\vert\boldsymbol{y},\tau\right)\propto p\left(\boldsymbol{y}\vert\boldsymbol{w},\sigma^{2}\right)p\left(\boldsymbol{w}\vert\tau\right)p\left(\sigma^{2}\right)\]
  </li>
  <li><strong>EM Algorithm</strong>  <a href="/assets/ml_files/08_Expectation_Maximization.pdf">slides</a>
    <ul>
      <li>A classic example of the use of the MAP</li>
      <li>Its use in clustering</li>
    </ul>

\[Q\left(\Theta\vert\Theta^{g}\right)=\sum_{\boldsymbol{y}\in\mathcal{Y}}\sum_{i=1}^{N}\log\left[\alpha_{y_{i}}p_{y_{i}}\left(x_{i}\vert\theta_{y_{i}}\right)\right]\prod_{j=1}^{N}p\left(y_{j}\vert x_{j},\Theta^{g}\right)\]
  </li>
  <li><strong>Feature Selection</strong> <a href="/assets/ml_files/09_Feature_Selection.pdf">slides</a>
    <ul>
      <li>Introduction to the curse of dimensionality</li>
      <li>Normalization the classic methods</li>
      <li>Data imputation using EM and Matrix Completion</li>
      <li>Methods for Subset Selection</li>
      <li>Shrinkage methods, the classic LASSO</li>
    </ul>

\[\widehat{\boldsymbol{w}}^{LASSO}=\arg\min_{\boldsymbol{w}}\left\{ \sum_{i=1}^{N}\left(y_{i}-\boldsymbol{x}^{T}\boldsymbol{w}\right)^{2}+\lambda\sum_{i=1}^{d}\left|w_{i}\right|^{q}\right\} \mbox{ with }q\geq0\]
  </li>
  <li><strong>Feature Generation</strong> <a href="/assets/ml_files/10_Feature_Generation.pdf">slides</a>
    <ul>
      <li>Introduction</li>
      <li>Fisher Linear Discriminant</li>
      <li>Principal Component Analysis</li>
      <li>Singular Value Decomposition</li>
    </ul>

\[L\left(\boldsymbol{u}_{2},\lambda_{1},\lambda_{2}\right)=\boldsymbol{u}_{2}^{T}S\boldsymbol{u}_{2}-\lambda_{1}\left(\boldsymbol{u}_{2}^{T}\boldsymbol{u}_{2}-1\right)-\lambda_{2}\left(\boldsymbol{u}_{2}^{T}\boldsymbol{u}_{1}-0\right)\]
  </li>
  <li><strong>Measures of Accuracy</strong> <a href="/assets/ml_files/11_Measures_Of_Accuracy.pdf">slides</a>
    <ul>
      <li>The alpha beta errors</li>
      <li>The Confusion Matrix</li>
      <li>The ROC curve</li>
    </ul>

    <p><img src="/assets/ml_files/images/ROC_Curve.png" alt="Interface" height="40%" width="40%" /></p>
  </li>
  <li><strong>Hidden Markov Models</strong> <a href="/assets/ml_files/13_Hidden_Markov_Models.pdf">slides</a>
    <ul>
      <li>Another classic example of the use of Dynamic Programming and EM</li>
      <li>The Three Problems</li>
    </ul>

\[\hat{L}\left(\lambda,\lambda^{n}\right)= \hat{Q}\left(\lambda,\lambda^{n}\right)-\lambda_{\pi}\left(\sum_{i=1}^{N}\pi_{i}-1\right)-\sum_{i=1}^{N}\lambda_{a_{i}}\left(\sum_{j=1}^{N}a_{ij}-1\right)-\sum_{i=1}^{N}\lambda_{b_{i}}\left(\sum_{k=1}^{M}b_{i}\left(k\right)-1\right)\]
  </li>
  <li><strong>Support Vector Machines</strong> <a href="/assets/ml_files/12_Support_Vector_Machines.pdf">slides</a>
    <ul>
      <li>The idea of margins</li>
      <li>Using the dual solution</li>
      <li>The kernel trick</li>
      <li>The soft margins</li>
    </ul>

\[Q(\alpha)={\displaystyle \sum_{i=1}^{N}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}d_{i}d_{j}\boldsymbol{x}_{j}^{T}\boldsymbol{x}_{i}}\]
  </li>
  <li><strong>The Perceptron</strong>  <a href="/assets/ml_files/14_Perceptron.pdf">slides</a>
    <ul>
      <li>The first discrete neural network</li>
      <li>The Idea of Learning</li>
    </ul>

\[y\left(i\right)=v\left(i\right)=\sum_{i=1}^{m}w_{k}\left(i\right)x_{k}\left(i\right)\]
  </li>
  <li><strong>Multilayer Perceptron</strong> <a href="/assets/ml_files/15_Multilayer_Perceptron.pdf">slides</a>
    <ul>
      <li>The Xor Problem</li>
      <li>The Hidden Layer</li>
      <li>Backpropagation for the new architecture</li>
      <li>Heuristic to improve the performance</li>
    </ul>

\[\triangle w_{kj}=\eta\delta_{k}y_{j}=\eta\left(t_{k}-z_{k}\right)f'\left(net_{k}\right)y_{j}\]
  </li>
  <li><strong>The Universal Representation Theorem</strong> <a href="/assets/ml_files/16_Universal_Approximation_Theorem.pdf">slides</a>
    <ul>
      <li>Cybenko Theorem</li>
    </ul>

\[G\left(\boldsymbol{x}\right)=\sum_{j=1}^{N}\alpha_{j}f\left(\boldsymbol{w}^{T}\boldsymbol{x}+\theta_{j}\right)\]
  </li>
  <li><strong>Convolutional Networks</strong> <a href="/assets/ml_files/17_Convolutional_Networks.pdf">slides</a>
    <ul>
      <li>Introduction to the image locality problem</li>
      <li>How convolutions can solve this problems</li>
      <li>Backpropagation on the CNN</li>
    </ul>

\[\left(f*g\right)\left[x,y\right]=\sum_{k=-n}^{n}\sum_{l=-n}^{n}f\left(k,l\right)g\left(x-k,y-l\right)\]
  </li>
  <li><strong>Regression and Classification Trees</strong> <a href="/assets/ml_files/18_Supervised_Decision_Trees.pdf">slides</a>
    <ul>
      <li>Using decision trees for Regression</li>
      <li>The Classification Tree</li>
      <li>Entropy to build the Classification Tree</li>
    </ul>

\[\Delta I\left(t\right)=I\left(t\right)-\frac{N_{tY}}{N_{t}}I\left(t_{Y}\right)-\frac{N_{tN}}{N_{t}}I\left(t_{N}\right)\]
  </li>
  <li><strong>Vapnik-Chervonenkis Dimensions</strong> <a href="/assets/ml_files/19_Vapnik_Chervonenkis_Dimension.pdf">slides</a>
    <ul>
      <li>Can we learn?</li>
      <li>The Shattering of the space</li>
      <li>The Inequality</li>
      <li>How to measure the power of a classifier</li>
    </ul>

\[E_{in}\left(g\right)&lt;E_{out}\left(g\right)+\sqrt{\frac{2k}{N}\ln\frac{eN}{k}}+\sqrt{\frac{1}{2N}\ln\frac{1}{\delta}}\]
  </li>
  <li><strong>Combining Models and Boosting</strong>  <a href="/assets/ml_files/20_Combining_Models.pdf">slides</a>
    <ul>
      <li>Bagging</li>
      <li>Mixture of Experts</li>
      <li>AdaBoosting</li>
    </ul>

    <p><img src="/assets/ml_files/images/alpha_m.png" alt="Interface" height="40%" width="40%" /></p>
  </li>
  <li><strong>Boosting Trees, XBoost and Random Forrest</strong> <a href="/assets/ml_files/21_Boosting_Trees_and_Random_Forest.pdf">slides</a>
    <ul>
      <li>Using Boosting in Trees</li>
      <li>Random Forrest</li>
      <li>Taylor approximation for Boosting Trees</li>
    </ul>

\[\mathcal{L}^{\left(t\right)}\simeq\sum_{i=1}^{N}\left[g_{i}f_{t}\left(\boldsymbol{x}_{i}\right)+\frac{1}{2}h_{i}f_{t}^{2}\left(\boldsymbol{x}_{i}\right)\right]+\Omega\left(f_{t}\right)\]
  </li>
  <li><strong>Introduction to Clustering</strong>  <a href="/assets/ml_files/22_Introduction_Clustering.pdf">slides</a>
    <ul>
      <li>The idea of finding patterns in the data</li>
      <li>The need for a similarity for the data</li>
      <li>The different features</li>
    </ul>

    <p><img src="/assets/ml_files/images/clustering.png" alt="Interface" height="40%" width="40%" /></p>
  </li>
  <li><strong>K-Means, K-Center and K-Meoids</strong>  <a href="/assets/ml_files/23_KMeans_KCenter_KMeoids.pdf">slides</a>
    <ul>
      <li>The NP-Problem of Clustering</li>
      <li>Using Cost functions for finding Clusters</li>
      <li>Using Approximation Algorithms for Clustering</li>
      <li>Beyond the metric space</li>
    </ul>

\[\sum_{k=1}^{N}\sum_{i:\boldsymbol{x}_{i}\in C_{k}}\left\Vert \boldsymbol{x}_{i}-\boldsymbol{\mu}_{k}\right\Vert ^{2}=\sum_{k=1}^{N}\sum_{i:\boldsymbol{x}_{i}\in C_{k}}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{k}\right)^{T}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{k}\right)\]
  </li>
  <li><strong>Hierarchical Clustering and Clustering for Large Data Sets</strong>  <a href="/assets/ml_files/24_Hierarchical_Clustering_Large_Data_Set_Clustering.pdf">slides</a>
    <ul>
      <li>Introduction</li>
      <li>The idea of nesting</li>
      <li>Bottom-Up Strategy</li>
      <li>Top-Down Strategy</li>
      <li>Large Data Set Clustering: CURE and DBASE</li>
    </ul>

    <p><img src="/assets/ml_files/images/dbscan.png" alt="Interface" height="40%" width="40%" /></p>
  </li>
  <li><strong>Cluster Validity</strong> <a href="/assets/ml_files/25_Unsupervised_Cluster_Validity.pdf">slides</a>
    <ul>
      <li>An Introduction to cluster validity</li>
    </ul>

\[W\left(\theta\right)=P\left(q\in\overline{D}_{\rho}\vert\theta\in\Theta_{1}\right)\]
  </li>
  <li><strong>Associative Rules</strong> <a href="/assets/ml_files/26_Associative_Rules.pdf">slides</a>
    <ul>
      <li>From the era of warehouses, finding frequent rules in databases</li>
    </ul>

    <p><img src="/assets/ml_files/images/PowerSet.png" alt="Interface" height="40%" width="40%" /></p>
  </li>
  <li><strong>Locality Sensitive Hashing</strong> <a href="/assets/ml_files/27_Locality_Sensitive_Hashing.pdf">slides</a>
    <ul>
      <li>
        <p>Hashing to find similar elements</p>

        <p><img src="/assets/ml_files/images/similarity.png" alt="Interface" height="40%" width="40%" /></p>
      </li>
    </ul>
  </li>
  <li><strong>Page Rank</strong> <a href="/assets/ml_files/28_PageRank.pdf">slides</a>
    <ul>
      <li>The Web as a Stochastic Matrix</li>
      <li>The Ranking as probabilistic vector</li>
      <li>The Power Method for finding the vector distribution</li>
    </ul>

\[A=\beta M+(1-\beta)\frac{1}{n}\mathbf{e}\cdot\mathbf{e^{T}}\]
  </li>
  <li><strong>Semi-supervised Learning</strong> <a href="/assets/ml_files/29_Semisupervised_Learning.pdf">slides</a>
    <ul>
      <li>The Basic of Semi-supervised Learning</li>
      <li>Using it on document labeling</li>
    </ul>

\[P\left(\boldsymbol{x}_{i}\vert\theta\right)=P\left(\left|\boldsymbol{x}_{i}\right|\right)\sum_{j\in\left\{ 1,2,...,M\right\} }P\left(c_{j}\vert\theta\right)\prod_{w_{t}\in\mathfrak{X}}P\left(w_{t}\vert c_{j},\theta\right)^{x_{it}}\]
  </li>
</ol>

<h1 id="book-chapters-on-machine-learning">Book Chapters on Machine Learning</h1>
<p>Here the book chapters based on these slides</p>

<ol>
  <li>
    <p><a href="/assets/ml_files/02_Notes_A_Basic_Introduction_to_Learning.pdf">An Introduction to Learning</a></p>
  </li>
  <li>
    <p>Linear Models</p>
  </li>
</ol>

<p><strong>UNDER CONSTRUCTION</strong></p>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Andres Mendez-Vazquez, A Personal Exploration about Intelligent Systems</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Andres Mendez-Vazquez, A Personal Exploration about Intelligent Systems</li><li><a class="u-email" href="mailto:kajuna0kajuna@gmail.com">kajuna0kajuna@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/kajuna0amendez"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">kajuna0amendez</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Exploring subjects on Machine Learning, Deep Learning and Computational Mathematics</p>
      </div>
    </div>

  </div>

</footer>

<!-- for mathjax support -->

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

</body>

</html>
