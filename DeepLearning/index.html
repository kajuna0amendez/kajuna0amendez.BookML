<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Deep Learning | Andres Mendez-Vazquez, A Personal Exploration about Intelligent Systems</title>
<meta name="generator" content="Jekyll v4.0.1" />
<meta property="og:title" content="Deep Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Exploring subjects on Machine Learning, Deep Learning and Computational Mathematics" />
<meta property="og:description" content="Exploring subjects on Machine Learning, Deep Learning and Computational Mathematics" />
<link rel="canonical" href="http://localhost:4000/DeepLearning/" />
<meta property="og:url" content="http://localhost:4000/DeepLearning/" />
<meta property="og:site_name" content="Andres Mendez-Vazquez, A Personal Exploration about Intelligent Systems" />
<script type="application/ld+json">
{"@type":"WebPage","url":"http://localhost:4000/DeepLearning/","headline":"Deep Learning","description":"Exploring subjects on Machine Learning, Deep Learning and Computational Mathematics","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Andres Mendez-Vazquez, A Personal Exploration about Intelligent Systems" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Andres Mendez-Vazquez, A Personal Exploration about Intelligent Systems</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/MachineLearning/">Introduction to Machine Learning</a><a class="page-link" href="/AdvancedMachineLearning/">Advanced Machine Learning</a><a class="page-link" href="/MLBusiness/">Machine Learning and Business</a><a class="page-link" href="/DeepLearning/">Deep Learning</a><a class="page-link" href="/Manifoldlearning/">Manifold Learning and Information Geometry</a><a class="page-link" href="/ArtificialIntelligence/">Introduction to Artificial Intelligence</a><a class="page-link" href="/AnalysisAlgorithms/">Analysis of Algorithms</a><a class="page-link" href="/LinearAlgebra/">Linear Algebra for AI</a><a class="page-link" href="/Probability/">Probability for AI</a><a class="page-link" href="/Optimization/">Optimization for AI</a><a class="page-link" href="/Projects/">Personal Projects</a><a class="page-link" href="/TechnicalPapers/">Technical Papers</a><a class="page-link" href="/Articles/">Publications</a><a class="page-link" href="/Students/">Past Students</a><a class="page-link" href="/Bio/">About Us</a></div>
      </nav></div>
</header>

<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Deep Learning</h1>
  </header>

  <div class="post-content">
    <p>Here, we have some of my attempts to interpret the field of Deep Learning</p>

<ol>
  <li><strong>Introduction</strong> <a href="/assets/nn_files/01_Introduction.pdf">Slides</a>
    <ul>
      <li>Introduction</li>
      <li>The Neural Architecture</li>
      <li>Types of activation functions</li>
      <li>McCulloch-Pitts model</li>
      <li>Vanishing Gradient</li>
    </ul>

    <p><img src="/assets/nn_files/images/ml_vanishing_gradient.png" alt="Interface" height="60%" width="60%" /></p>
  </li>
  <li><strong>Neural Networks as graphs</strong>
    <ul>
      <li>Examples</li>
      <li>Architectures</li>
      <li>Design of neural networks</li>
      <li>Representing knowledge on a Neural Network</li>
    </ul>

    <p><img src="/assets/nn_files/images/recurrent_nn.png" alt="Interface" height="30%" width="30%" /></p>
  </li>
  <li><strong>Learning</strong>  <a href="/assets/nn_files/02_Learning_Process.pdf">Slides</a>
    <ul>
      <li>Introduction</li>
      <li>Error correcting learning</li>
      <li>Memory based Learning</li>
      <li>Hebbian Learning</li>
      <li>Competitive Learning</li>
      <li>Boltzmann Learning</li>
    </ul>

\[f\left(x_{t}\right)=\begin{cases}
w_{t}\times\sigma\left(x_{t}\right) &amp; \mbox{ if }x_{t}&gt;t\mbox{ and }w_{t+1}=\alpha w_{t}\mbox{ with } \alpha&gt;1\\
w_{t}\times\sigma\left(x_{t}\right) &amp; \mbox{ if }x_{t}&gt;t\mbox{ and }w_{t+1}=\alpha w_{t}\mbox{ with } \alpha&lt;1
\end{cases}\]
  </li>
  <li><strong>Perceptron</strong> <a href="/assets/nn_files/03_Introduction_Single_Layer_Perceptron.pdf">Slides</a>
    <ul>
      <li>History and the beginning as PDE</li>
      <li>Adaptive Filtering</li>
      <li>Rosenblattâ€™s algorithm</li>
    </ul>

    <p><img src="/assets/nn_files/images/perceptron.png" alt="Interface" height="60%" width="60%" /></p>
  </li>
  <li><strong>Multilayer Perceptron</strong> <a href="/assets/nn_files/04_Multilayer_Perceptron.pdf">Slides</a>
    <ul>
      <li>Solving the XOR problem</li>
      <li>The basic architecture</li>
      <li>Backpropagation</li>
      <li>Matrix form of the backpropagation</li>
      <li>The Universal Approximation Theorem</li>
    </ul>

    <p><img src="/assets/nn_files/images/MultiLayerPerceptron.png" alt="Interface" height="40%" width="40%" /></p>
  </li>
  <li><strong>Deep Forward Networks</strong> <a href="/assets/nn_files/05_DeepForward_Networks.pdf">Slides</a>
    <ul>
      <li>The problem with shallow architectures - lack of expressiveness</li>
      <li>From simple features to complex ones</li>
      <li>Component of Deep Forward Architectures</li>
      <li>The Problems with the Gradient in Deeper Architectures</li>
      <li>RELU a possible solution</li>
      <li>Examples of Deep Architectures: Generative, Residual, Autoencoders, Boltzmann Machines, etc</li>
    </ul>

    <p><img src="/assets/nn_files/images/autoencoders.png" alt="Interface" height="40%" width="40%" /></p>
  </li>
  <li><strong>The idea of Back-propagation and Automatic Differentiation</strong> <a href="/assets/nn_files/06_Backpropagation_Automatic_Differentiation.pdf">Slides</a>
    <ul>
      <li>Derivation of Network Functions</li>
      <li>Function Composition</li>
      <li>The Rule Chain AKA Backpropagation</li>
      <li>Advantages of Automatic Differentiation</li>
      <li>Forward and Reverse Method</li>
      <li>Proving the Reverse Method</li>
      <li>Basic Implementation of Automatic Differentiation</li>
    </ul>

\[A_{i}\equiv\left[\begin{array}{ccccccc}
1 &amp; 0 &amp; \ldots &amp; 0 &amp; \ldots &amp; \ldots &amp; 0\\
0 &amp; 1 &amp; \ldots &amp; 0 &amp; \ldots &amp; \ldots &amp; 0\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \ldots &amp; \ldots\\
0 &amp; 0 &amp; \ldots &amp; 1 &amp; \ldots &amp; \ldots &amp; 0\\
c_{i1-n} &amp; c_{i2-n} &amp; \ldots &amp; c_{ii-n} &amp; \ldots &amp; \ldots &amp; 0\\
0 &amp; 0 &amp; \ldots &amp; 0 &amp; 1 &amp; \ldots &amp; 0\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\
0 &amp; 0 &amp; \ldots &amp; \ldots &amp; \ldots &amp; \ldots &amp; 1
\end{array}\right]\in\mathbb{R}^{\left(n+l\right)\times\left(n+l\right)}\]
  </li>
  <li><strong>Stochastic Gradient Descent</strong> <a href="/assets/nn_files/07_Stochastic_Gradient_Descent.pdf">Slides</a>
    <ul>
      <li>Review Gradient Descent</li>
      <li>The problem with Large Data sets</li>
      <li>Convergence Rate</li>
      <li>Accelerate the Gradient Descent: Nesterov</li>
      <li>Robbins-Monro idea</li>
      <li>SGB vs BGD</li>
      <li>The Minbatch</li>
      <li>Least-Mean Squares Adaptive</li>
      <li>AdaGrad</li>
      <li>ADAM</li>
    </ul>

\[\sum_{k=1}^{t}\frac{\widehat{m}_{k}}{\left(\sqrt{\widehat{v}_{k}}\right)}=\sum_{k=1}^{t}\frac{\frac{m_{k}}{\left(1-\beta_{1}^{k}\right)}}{\left(\sqrt{\frac{v_{k}}{\left(1-\beta_{2}^{k}\right)}}\right)}=\sum_{k=1}^{t}\frac{\left(1-\beta_{2}^{k}\right)^{\frac{1}{2}}}{\left(1-\beta_{1}^{k}\right)}\times\frac{m_{k}}{\sqrt{v_{k}}}\]
  </li>
  <li><strong>Introduction to Recurrent Neural Networks</strong>  <a href="/assets/nn_files/08_Recurrent_Neural_Networks.pdf">Slides</a>
    <ul>
      <li>Vanilla RNN</li>
      <li>The Training Problem</li>
      <li>Backpropagation Though Time (BPTT)</li>
      <li>Dealing with the problem LSTM and GRU</li>
      <li>Can we avoid the BPTT?</li>
    </ul>

    <p><img src="/assets/nn_files/images/unfolding.png" alt="Interface" height="60%" width="60%" /></p>
  </li>
  <li><strong>Loss Functions</strong>
    <ul>
      <li>The Bayesian Loss Function</li>
      <li>Utility and loss</li>
      <li>The Quadratic Loss</li>
      <li>The problem with it</li>
      <li>The Logistic and 0-1 Loss</li>
      <li>Alternatives</li>
      <li>More Complex Loss Functions</li>
      <li>Adaptive Loss Functions</li>
    </ul>

\[L=-\sum_{i=1}^{C}z_{i}\log\left(f\left(y_{i}\right)\right)=-log\left(\frac{\exp\left\{ y_{p}\right\} }{\sum_{j=1}^{C}\exp\left\{ y_{p}\right\} }\right)\]
  </li>
  <li><strong>Regularization in Deep Neural Networks</strong> <a href="/assets/nn_files/09_Regularization_in_Neural_Networks.pdf">Slides</a>
    <ul>
      <li>Bias-Variance Dilemma</li>
      <li>The problem of overfitting</li>
      <li>Methods of regularization in Deep Neural Networks
        <ul>
          <li>Dropout</li>
          <li>Random Dropout Probability</li>
          <li>Batch Normalization</li>
        </ul>
      </li>
    </ul>

\[y_{i}=BN_{\gamma,\beta}\left(\boldsymbol{x}_{i}\right)\]
  </li>
  <li><strong>Convolutional Networks</strong> <a href="/assets/nn_files/10_Convolutional_Networks.pdf">Slides</a>
    <ul>
      <li>The problem of the translation on images</li>
      <li>The need of locality</li>
      <li>The Convolutional Operator</li>
      <li>Convolutional Networks</li>
      <li>Layers in Convolutional Networks</li>
      <li>An Example</li>
    </ul>

    <p><img src="/assets/nn_files/images/convolutions.png" alt="Interface" height="50%" width="50%" /></p>
  </li>
  <li>
    <p><strong>Boltzmann Machines</strong></p>

\[\]
  </li>
  <li>
    <p><strong>Autoencoders</strong></p>

\[\]
  </li>
  <li>
    <p><strong>Evaluation of Deep Neural Networks</strong></p>

\[\]
  </li>
  <li>
    <p><strong>Generative Adversarial Networks</strong></p>

\[\]
  </li>
  <li>
    <p><strong>Transfer Learning</strong></p>

\[\]
  </li>
  <li>
    <p><strong>Deep Residual Networks</strong></p>

\[\]
  </li>
  <li>
    <p><strong>Second Order Methods</strong></p>

\[\]
  </li>
  <li>
    <p><strong>Partial Differential Equations in Deep Learning</strong></p>

\[\]
  </li>
</ol>

<p>UNDER CONSTRUCTION</p>


  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Andres Mendez-Vazquez, A Personal Exploration about Intelligent Systems</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Andres Mendez-Vazquez, A Personal Exploration about Intelligent Systems</li><li><a class="u-email" href="mailto:kajuna0kajuna@gmail.com">kajuna0kajuna@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/kajuna0amendez"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">kajuna0amendez</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Exploring subjects on Machine Learning, Deep Learning and Computational Mathematics</p>
      </div>
    </div>

  </div>

</footer>

<!-- for mathjax support -->

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

</body>

</html>
